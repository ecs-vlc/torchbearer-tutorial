{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Optimisation and torchbearer Metrics\n",
    "\n",
    "We've already seen how to train a simple CNN with torchbearer, lets now have a look at something slightly different, optimising a simple function. \n",
    "\n",
    "Recall the tutorial Jon gave on Autograd recently ([here](https://github.com/ecs-vlc/understanding-autograd)), we shall be optimising the function in the final section [here](https://render.githubusercontent.com/view/ipynb?commit=e9da73fbe13b709989bcd066492e1058bae0aab2&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6563732d766c632f756e6465727374616e64696e672d6175746f677261642f653964613733666265313362373039393839626364303636343932653130353862616530616162322f5079546f72636841442e6970796e62&nwo=ecs-vlc%2Funderstanding-autograd&path=PyTorchAD.ipynb&repository_id=141979756&repository_type=Repository#Gradient-descent-&-gradients-with-respect-to-a-vector).\n",
    "\n",
    "This function is: \n",
    "\n",
    "$f(x) = x_1^2 + x_2^2 + x_3^2$\n",
    "\n",
    "with expected minimum at $x = (0,0,0)$.\n",
    "\n",
    "## Structure\n",
    "\n",
    "To do this we will need four things:\n",
    "- a pytorch model that represents the function to be optimised\n",
    "- an optimser bound to the model to update it\n",
    "- a measure of loss\n",
    "- a torchbearer metric to view progress\n",
    "\n",
    "## Model\n",
    "\n",
    "Lets start with the model. \n",
    "We put $x$ as a member variable for the model and tell PyTorch that we want to optimise it by setting it as a [Parameter](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html?highlight=parameter). \n",
    "Can you implement the forward pass to return $f(x)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "import torchbearer as tb\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.x = torch.nn.Parameter(x)\n",
    "\n",
    "    def f(self):\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "        \n",
    "    def forward(self, _):\n",
    "        return self.f()\n",
    "    \n",
    "p = torch.tensor([2.0, 1.0, 10.0])\n",
    "model = Net(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser\n",
    "\n",
    "For this example, we shall use SGD to optimise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We have a very simple loss for this problem, the value of the function, since this is what we want to minimise (and is non-negative). As in pytorch, torchbearer losses are a function of y_pred and y_true, the predictions of the network and the targets respectively. In this example we do not have targets so we can ignore that variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y_true):\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchbearer Metrics\n",
    "\n",
    "Metrics form the main output of a torchbearer trial. They are logged when logging callbacks are used and they are displayed when printer callbacks are used.\n",
    "\n",
    "Below is an example of a simple metric that inherits the base metric class which accepts a name on init, and outputs on process the current x value, which it grabs from the pytorch model instance. Metrics must output a dict so they can be shown and logged, for this a decorator is provided that just wraps the output of process with a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tb.metrics.to_dict\n",
    "class est(tb.metrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__('x') # Named 'x'\n",
    "\n",
    "    def process(self, state):\n",
    "        return state[tb.MODEL].x.data # Value = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can now easily perform optimisation for 10000 steps whilst printing the current estimate for the minimum. Note that since we have no data in this case, we use [for_training_steps](https://torchbearer.readthedocs.io/en/latest/code/main.html#torchbearer.trial.Trial.for_steps) instead of providing the trial a generator. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1(t): 100%|██████████| 10000/10000 [00:17<00:00, 574.01it/s, est=tensor([0.0001, 0.0000, 0.0005], device='cuda:0'), running_loss=2.3e-07, loss=5.25, loss_std=15.8]\n"
     ]
    }
   ],
   "source": [
    "training_steps = 10000\n",
    "tbtrial = tb.Trial(model, optim, loss, metrics=[est(), 'loss']).for_train_steps(training_steps).to('cpu')\n",
    "_ = tbtrial.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should show the function converging to the correct minimum.\n",
    "\n",
    "## Exercise: \n",
    "\n",
    "Implement the function to optimise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on\n",
    "\n",
    "The next notebook is the VAE notebook where we'll run a VAE and look a bit more at constructing callbacks, for example to visualise results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
