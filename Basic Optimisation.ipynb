{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Optimisation and torchbearer Metrics\n",
    "\n",
    "We've already seen how to train a simple CNN with torchbearer, lets now have a look at something slightly different, optimising a simple function. \n",
    "\n",
    "Recall the tutorial Jon gave on Autograd recently ([here](https://github.com/ecs-vlc/understanding-autograd)), we shall be optimising the funciton in the final section [here](https://render.githubusercontent.com/view/ipynb?commit=e9da73fbe13b709989bcd066492e1058bae0aab2&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6563732d766c632f756e6465727374616e64696e672d6175746f677261642f653964613733666265313362373039393839626364303636343932653130353862616530616162322f5079546f72636841442e6970796e62&nwo=ecs-vlc%2Funderstanding-autograd&path=PyTorchAD.ipynb&repository_id=141979756&repository_type=Repository#Gradient-descent-&-gradients-with-respect-to-a-vector).\n",
    "\n",
    "This function is: \n",
    "\n",
    "$f(x) = x_1^2 + x_2^2 + x_3^2$\n",
    "\n",
    "with expected minimum at $x = (0,0,0)$.\n",
    "\n",
    "## Structure\n",
    "\n",
    "To do this we will need four things:\n",
    "- a pytorch model that represents the function to be optimised\n",
    "- an optimser bound to the model to update it\n",
    "- a measure of loss\n",
    "- a torchbearer metric to view progress\n",
    "\n",
    "## Model\n",
    "\n",
    "Lets start with the model. \n",
    "We put $x$ as a member variable for the model and tell PyTorch that we want to optimise it by setting it as a [Parameter](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html?highlight=parameter). \n",
    "The forward pass then evaluates $f(x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "import torchbearer as tb\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.pars = torch.nn.Parameter(x)\n",
    "\n",
    "    def f(self):\n",
    "        out = torch.zeros_like(self.pars)\n",
    "        out[0] = self.pars[0]\n",
    "        out[1] = self.pars[1]\n",
    "        out[2] = self.pars[2]\n",
    "        return torch.sum(out**2)\n",
    "\n",
    "    def forward(self, _, state):\n",
    "        return self.f()\n",
    "    \n",
    "p = torch.tensor([2.0, 1.0, 10.0])\n",
    "model = Net(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser\n",
    "\n",
    "For this example, we shall use SGD to optimise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We have a very simple loss for this problem, the value of the function, since this is what we want to minimise (and is non-negative). As in pytorch, torchbearer losses are a function of y_pred and y_true, the predictions of the network and the targets respectively. In this example we do not have targets so we can ignore that variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y_true):\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchbearer Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tb.metrics.to_dict\n",
    "class est(tb.metrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__('est') # Named 'est'\n",
    "\n",
    "    def process(self, state):\n",
    "        return state[tb.MODEL].pars.data # Value = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can now easily perform optimisation for 10000 steps whilst printing the current estimate for the minimum. Note that since we have no data in this case, we use [for_training_steps]() instead of providing the trial a generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1(t): 100%|██████████| 10000/10000 [00:20<00:00, 491.01it/s, est=tensor([4.0813e-09, 2.0406e-09, 2.0406e-08], device='cuda:0'), running_loss=4.69e-16, loss=1.07e-08, loss_std=3.22e-08]\n"
     ]
    }
   ],
   "source": [
    "training_steps = 10000\n",
    "tbtrial = tb.Trial(model, optim, loss, metrics=[est(), 'loss'], pass_state=True).for_train_steps(training_steps).to('cuda')\n",
    "_ = tbtrial.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good, we found the correct minima. Can you modify this to optimise silly such as the intersection of two pdfs of [these](https://en.wikipedia.org/wiki/Kumaraswamy_distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
